%!TEX root = ../dissertation.tex

\chapter{Background and state of the art}
\label{chap:literature}
This Chapter explains the necessary background knowledge in \autoref{sec:bgknow} and comprises the literature review in the following Sections. The literature review includes the protocol in \autoref{sec:lit_protocol}, \acrshort{fair} principles in general and for research software in \autoref{sec:lit_fairgeneral} and \autoref{sec:lit_fairrs}, and existing GitHub analyses of FAIRness and research domains in \autoref{sec:lit_github}.

\section{Technical background knowledge}
\label{sec:bgknow}
\textit{GitHub} is a development platform from which data for this study is collected. The platform allows developers and organizations to collaborate in open or private repositories based on the version control system Git \cite{chacon2014pro}. Common ways for collaboration are creating \textit{issues} or \textit{pull requests}. Issues are used to track feedback, ideas, issues like bugs, feature requests and more. Pull requests allow users to commit any changes to the repository that a user would like to suggest. 
This is usually done by \textit{forking} a repository, which essentially means cloning. Afterwards, the contributor \textit{commits} and \textit{pushes} changes on the forked repository and \textit{merges} the changes to the original one via pull requests. It also offers the opportunity to follow people to get notified about their activity.
GitHub also offers many other features for free, like \textit{GitHub Actions}, which allows for integrated \acrfull{cicd} pipelines, and automatic detection of citation and license files, leading to machine-readable metadata. There is also a \textit{REST API} \cite{github_rest} which allows us to extract information about users and repositories with all accompanying metadata that is available. Such REST APIs use the HTTP protocol to receive and modify data. To make things easier, we use ghapi \cite{noauthor_ghapi_nodate} in the \acrshort{swordsuu} framework, a Python wrapper for the GitHub REST API.


In the remainder of the thesis, we will use the terms \textit{metric} and \textit{(\acrshort{fair}) variable}. A variable is any kind of captured data like the owner of the repository or the repository id. A \acrshort{fair} variable is related to concepts of FAIRness, openness, and sustainability. Examples of \acrshort{fair} variables are the availability of a license or citation information. A \textit{metric} is any kind of numeric variable like the number of issues, forks or stargazers. A metric is always a variable and can also be a \acrshort{fair} variable, but a (\acrshort{fair}) variable does not always have to be a metric.

The \acrfull{fair4rs}-subgroup3 reviewed definitions of research software and provided their own definition, which will be used for the remainder of the study \cite{gruenpeter_defining_2021}:
\begin{quote}
\begin{definition}
Research Software includes source code files, algorithms, scripts, computational workflows and executables that were created during the research process or for a research purpose. Software components (e.g., operating systems, libraries, dependencies, packages, scripts, etc.) that are used for research but were not created during or with a clear research intent should be considered software in research and not Research Software. This differentiation may vary between disciplines. The minimal requirement for achieving computational reproducibility is that all the computational components (Research Software, software used in research, documentation and hardware) used during the research are identified, described, and made accessible to the extent that is possible. \cite{gruenpeter_defining_2021}
\end{definition}
\end{quote}
The creation of the definition has been a controversial discussion. During the analysis of determining whether a software is considered a research software, there were often conflicting opinions. Katz et al. \cite{katz_research_2020} also stated that it is hard to distinguish between research and non-research initiatives due to an increasing focus on open source software in general.

An essential aspect of the research software definition is the variation between disciplines. What exactly is determined as research software depends on the agreement within communities and may also change over time. This allows for flexibility since software is highly complex and makes the definition applicable in the future as well \cite{lamprecht_towards_2020}. Software can also have different granularity levels, making it difficult to determine and reference the appropriate level correctly \cite{research_data_allianceforce11_software_source_code_identification_wg_use_2020}. However, these aspects also make it difficult to categorize research software properly. Since the categorization is flexible, labelled data from today might not be correctly labelled in the future.



\section{Literature review protocol}
\label{sec:lit_protocol}

The relevant literature includes FAIR principles in general, specifically regarding research software and the analysis of open source GitHub repositories. To that effect, the snowball method \cite{wohlin_guidelines_2014} was applied for identified key papers \cite{lamprecht_towards_2020, wilkinson_fair_2016, katz_fair4rs_2021-1}. Both forward and backward snowballing were applied. Forward snowballing refers to following papers that have cited a specific paper, while backward snowballing refers to following the references in a specific paper. There are also reading lists containing relevant research published before and after February 2020 for the application of \acrshort{fair} principles for software that were taken into account \cite{noauthor_fair4software_2020, wg_fair4rs_2021}.
In addition, Google scholar was used to identify additional papers with the queries \textit{FAIR research software}, \textit{FAIR research software GitHub}, and \textit{GitHub analysis metadata open}. The snowball method was again applied to identified key papers \cite{hasselbring_open_2020}. 

\section{FAIR principles in general}
\label{sec:lit_fairgeneral}
The \acrshort{fair} principles were published in 2016 by Wilkinson et al. \cite{wilkinson_fair_2016} to clarify the objectives of good data management and are concise, domain-independent and high-level. Good data management acts as a pre-condition for better knowledge discovery and innovation. The initiative initially started in a workshop in 2014 with many involved parties interested in data discovery and reuse. 
The four foundational \acrshort{fair} principles aim to improve \textit{\textbf{F}indability, \textbf{A}ccessibility, \textbf{I}nteroperability, and \textbf{R}eusability}. They are accompanied by guiding principles that further elaborate on each foundational principle. A reference can be found in \autoref{AppendixA}, which also includes proposed \acrshort{fair} principles for research software.


An important aspect of the \acrshort{fair} principles is that they should apply to all digital research objects like algorithms, tools, workflows, and research software. To allow for reproducibility and reusability of research, it is necessary to make all relevant objects of the research process \acrshort{fair}. There has also been an increase in general-purpose data repositories, with Zenodo \cite{zenodo} being one well-known and widely used example. In contrast to special-purpose repositories, general-purpose ones lead to less integrated data ecosystems that reinforce issues in discovery and reusability. Scientific data gathering, therefore, is a time-consuming process that can be improved. The goal is to include aspects of FAIRness in the quality assessment of digital research objects.

Wilkinson et al. stated that the FAIRification of digital research objects leads toward machine-actionability. Machine-readability becomes more and more important as humans increasingly rely on computational agents for discovery and integration. In the optimal case, such a machine can autonomously explore digital objects for different tasks: Identify object type, determine usefulness for a given context, determine usability considering license, accessibility or usage constraints, and take appropriate action. Nonetheless, the \acrshort{fair} principles are aimed toward human-driven and machine-driven activities, which is different from similar initiatives. They also precede implementation and are technology-agnostic, allowing for high flexibility and applicability across domains. However, they are not a standard and primarily act as a guideline. The entry barrier for relevant operators is kept low by only minimally defining the \acrshort{fair} guiding principles. The foundational principles are also independent and separable, though they are still related.



In 2017, a follow-up paper \cite{mons_cloudy_2017} by the original authors of the \acrshort{fair} principles was released to clarify the intent and interpretation of the \acrshort{fair} Principles. They emphasized again that \acrshort{fair} principles focus on making research objects reusable to maximize their value without specifying technical requirements and that it is not a standard. Instead, they are permissive guidelines that can be used to develop flexible community standards \cite{boeckhout_fair_2018, mons_cloudy_2017}.
In addition, they stated that open science is steadily growing, and \acrshort{fair} principles within this context have been embraced by a wide range of governments and funding bodies. What is still needed are changes to science reward and methodological practice, as well as increased support infrastructure. Resources in such infrastructures should be \acrshort{fair}. For this purpose, community-acceptable rules of engagement should be developed that are suitable for a given community. They also clarify that “unfair” does not exist. For example, if data is non-machine-readable, it does not become unfair. Rather, FAIRness is a continuous spectrum. Making associated metadata \acrshort{fair}, in case the data may not be shared if it is for example personal data, already counts as full participation within the \acrshort{fair} ecosystem. This also showcases that openness and \acrshort{fair} are not the same. Data, especially health data, can often be on an individual level and thus resulting in privacy concerns which lead to mixed success regarding appeals for open data \cite{boeckhout_fair_2018, borgman_big_2015}. 
This is of less concern for \acrshort{fair} research software and one of the significant differences between data and software.
They introduce the term re-useless data for datasets that are not available for reuse, which is the majority of datasets with ~80\% \cite{mons_cloudy_2017}. The first step to making such datasets \acrshort{fair} is providing a \acrfull{pid}, but that alone is insufficient. The next step would be adding \acrshort{fair} metadata, followed by making the data \acrshort{fair}. The last step would then be to link the data with other \acrshort{fair} data to achieve the internet of \acrshort{fair} data. What can be taken away from this is that improving FAIRness is a continuous effort, and it is not clear when exactly something is \acrshort{fair}. 

There has been research on how \acrshort{fair} principles can be applied to other digital research objects such as computational workflows, training material, machine learning tools and models \cite{wolf_reusability_2021, garcia_ten_2020, katz_working_2021}. For machine learning specifically, Katz et al. \cite{katz_working_2021} started a community-building process for applying \acrshort{fair} concepts to that domain. The \acrshort{fair4rs}-subgroup2 was tasked with exploring \acrshort{fair} in contexts other than data and software \cite{kuzak_fair4rs_nodate}. Common recommendations based on their analysis include rich metadata (e.g. citation), recognition, proper documentation, unique identification of material (e.g. via \acrshort{pid}s), and getting community endorsement. We will see in \autoref{sec:lit_fairrs} that these recommendations are also highly relevant and applicable to research software.



\section{FAIR principles for Research Software}
\label{sec:lit_fairrs}
Since the original \acrshort{fair} principles were published, several publications have pointed out that these principles do not directly translate to research software \cite{gruenpeter_morane_m215_2020, hasselbring_fair_2020, lamprecht_towards_2020}. Thus, it is necessary to modify, extend, add and delete these principles to make them more applicable to research software.

The \acrfull{fair4rswg} is well-positioned to be the community forum regarding \acrshort{fair} principles for software, services and workflows \cite{noauthor_six_nodate}. The \acrshort{fair4rswg} was assembled by the \acrfull{resa} \cite{alliance_task_2020}, the \acrfull{force11} \cite{force11_members}, and the \acrfull{rda} \cite{RDABerman2020Research}. They aim to represent many different stakeholders, including research software users, developers, maintainers, policy creators, managers of relevant infrastructure (e.g. publishers, archives), and research (software) funders \cite{katz_fair4rs_2021}. In addition, mature wide-scale community engagement is promoted by basing it on the \acrfull{cscce} participation model which enables structured member engagement \cite{woodley_cscce_2020}. This working group created four subgroups that were assigned different tasks in order to support the creation of community-endorsed \acrshort{fair4rs} principles \cite{chue_hong_fair_2021, chue_hong_fair_2022}:
\begin{enumerate}
    \item Examination of \acrshort{fair} Guiding Principles for research software from scratch \cite{katz_fresh_2021}. A group of people, many of whom are involved in the \acrshort{fair4rswg}, previously published the position paper “Towards \acrshort{fair} principles for research software” \cite{lamprecht_towards_2020} aimed at developing a set of \acrshort{fair} principles for research software. This subgroup took a fresh look at the same problem, and therefore planned to exclude authors of that paper\footnote{In the end, authors of that paper were included. The original idea, however, was to exclude them.}. They also did not look at previous research except for the original \acrshort{fair} principles. 
    \item Application of \acrshort{fair} Guiding Principles in other contexts like workflows and training material \cite{kuzak_fair4rs_nodate}.
    \item Definition of research software to provide context \cite{gruenpeter_defining_2021}. This definition is used throughout the study.
    \item Review of new research related to \acrshort{fair} Software since the release of the paper “Towards \acrshort{fair} principles for research software”, as well as the paper itself \cite{lamprecht_towards_2020, wg_fair4rs_2021, chue_hong_what_2021}. 
\end{enumerate}
Three additional subgroups were launched afterwards in September 2021 \cite{chue_hong_fair4rs_2022} to review adoption guidelines for \acrshort{fair4rs} principles \cite{martinez_survey_2022}, (early) adoption support \cite{martinez-ortiz_fair4rs_2022}, and governance \cite{honeyman_subgroup_nodate}.  
There are also related initiatives whose main focus does not lie on \acrshort{fair} research software. However, since \acrshort{fair} research software is related to \acrshort{fair} data, they also contributed to this subject:
\begin{itemize}
    \item The CURE-FAIR Working Group stated in their charter \cite{cure-faircase} that their objective is to “establish standards-based guidelines for curating for reproducible and FAIR data and code”.
    \item FAIRsFAIR - Fostering Fair Data Practices in Europe - aims to supply practical solutions for the use of the FAIR data principles throughout the research data life cycle. Emphasis is on fostering FAIR data culture and the uptake of good practices in making data FAIR. The FAIRsFAIR Work Package 2 published an assessment report on “FAIRness of software” \cite{gruenpeter_m215_2020}. They focused on software as a research outcome.
    \item GO BUILD Pillar of GO FAIR US - they plan to develop a FAIR assessment framework for research software platforms, including badging which is a common practice in open source development \cite{noauthor_go_nodate}.
    \item The \acrfull{ardc} created a \acrshort{fair} self-assessment tool for data \cite{ardcfair} and is in the development of a similar tool for software \cite{ardcfairsoftware}.  
\end{itemize}

As previously mentioned, data can have privacy concerns which is typically not the case for research software. On the contrary, openness should be the norm for research software for many reasons related to facilitating open science, transparency, innovation through collaboration, reliability, and more. It should only be closed when appropriate reasoning for exceptional cases is given \cite{hasselbring_fair_2020}. Chue Hong et al. mentioned that research software should be “as open as possible, as closed as necessary" \cite{chue_hong_fair_2021}. It is also important for open science improvement to recognize research software as a first-class research output, similar to research data \cite{gruenpeter_software_2021, akhmerov_making_2019, chue_hong_making_2019, garcia_castro_software_2020}.

% Development of FAIR principles for research software %
Lamprecht et al. \cite{lamprecht_towards_2020} published the first position paper that suggests modified, extended and additional \acrshort{fair} principles for the application of these for research software. They explained why the application of \acrshort{fair} principles for research software needs to take different aspects into account, why quality considerations go beyond \acrshort{fair} and offer an analysis of where principles need adaptation or additional principles. 

Software shares some characteristics like the possibility of having a \acrfull{doi} assigned, which is one kind of a \acrshort{pid}, or having a license added, but also has significant differences. Data are facts, while software is the result of a creative process that is executable and usually consists of many dependencies with frequent updates, which leads to shorter lifetimes than for data. Research software can be made available on many platforms like digital repositories, archives, language-specific archive networks and more. 
In addition, research software is often \acrshort{foss} which overlaps with \acrshort{fair} regarding accessibility and reusability. \acrshort{fair} principles, however, do not require openness. Certain data is sensitive and therefore require access control which is different from research software as this is part of the methodology which is expected to be shared.

They argue that there needs to be a distinction between the form and function of research software since quality considerations differ. \acrshort{fair} principles cover the form with interoperability and reusability, but it needs to be mentioned that reusability also needs to take sustainability and the non-static nature of software into account, which is different to data. Maintainability improves the sustainable growth of software and includes modularity, understandability, testability, following guidelines and coding standards and can often be measured or quantified. Quality regarding function is more difficult to measure for many reasons, and there are ongoing discussions on whether applicable principles can be developed.

The original \acrshort{fair} principles and the proposed \acrshort{fair} principles for research software by Lamprecht et al. and Chue Hong et al. \cite{chue_hong_fair_2022} can be seen in \autoref{AppendixA}. The complete comparison of the different principles can be found in the appendix of Chue Hong et al. \cite{chue_hong_fair_2022}. 

Regarding \textbf{findability} for research software, the main concern is to ensure unambiguous identification by common search strategies. This includes search engines but also registries and repositories. In the following text, the principles proposed by Lamprecht et al. will be elaborated upon where necessary. Some principles are mostly rephrased to be suitable research software. For F1, a key difference is that research software needs a \acrshort{pid} for all software versions to guarantee reproducibility. While Git offers code version control, GitHub, which is often used for publishing and active development, does not serve the purpose of persistent storage. 
Zenodo is often used together with Github to solve this. 
For F2, typical metadata that describes software should at least include where to find specific versions, how to cite, authors of the research software, inputs and outputs, and dependencies. There are also ongoing projects that offer solutions for structured metadata annotation for software like codemeta \cite{noauthor_codemeta_2021}. It should also be noted that vocabulary provided by community-approved ontologies should be used where possible. An ontology is "a classification of categories describing a software artefact with explicit specifications of its entities and relationships in a certain domain of use" \cite{gruenpeter_morane_m215_2020}. Having such an ontology helps to gain a shared understanding of used terms. For F4, it is important to note that there are three classes of registries and repositories:
\begin{itemize}
    \item General ones like Zenodo, GitHub, and software archives like the Software Heritage project \cite{noauthor_opening_nodate}
    \item Language-specific ones like CRAN or PyPI
    \item Domain-specific ones like the bioinformatics-specific BioConductor \cite{noauthor_bioconductor_nodate}
\end{itemize}
Research software should be published in a suitable registry or repository. This may be influenced by programming language or operating systems that are standard within the respective community.

Regarding \textbf{accessibility}, the original principles apply, while there is also the additional aspect of the ability to actually use the software. Alternatively, in other words, to access the functionality, which as a minimum, requires the availability of a working version. This also means that the software should be downloadable and runnable, as it might depend on data samples, (paid) registration, other (proprietary) software and more. The authors also mentioned that accessibility, interoperability and (re)usability are intrinsically connected for research software and consider aspects of installation instructions, dependencies and licensing as part of other principles. Changes are relatively little compared to the original accessibility principles. Mainly, they are rephrased to include a reference to software.

Regarding \textbf{interoperability}, a definition from the IEEE Standard Glossary of Software Engineering Terminology was provided: “ability of two or more systems or components to exchange information and to use the information that has been exchanged” \cite{noauthor_ieee_1990}. This was complemented by a definition of semantic interoperability, ensuring “that these exchanges make sense - that the requester and the provider have a common understanding of the ‘meanings‘ of the requested services and data" \cite{heiler_semantic_1995}. Interoperability for research software is the most challenging high-level principle because software usually has high complexity. It is already the most challenging for data, which is usually less complex due to the static nature, while research software interacts with many components that also include other software and data. There were more modifications for the interoperability principles compared to the accessibility principles due to research software being live digital objects, unlike data. The authors provided three angles from which interoperability for software can be viewed at:
\begin{itemize}
    \item For a set of independent but interoperable objects to produce a runnable version
    \item For a stack of digital objects that should work together to execute a given task
    \item For workflows which interconnect different standalone software tools to transform data
\end{itemize}

One requirement for interoperability is software metadata, which provides valuable information. This should include software versions, dependencies with versions, data types, input and output format, communication interfaces, and deployment options. Software portability, which refers to the ability to run software in different operating systems, is also part of interoperability. Software containerization tools like Docker or Rocket help in improving software portability. For I1, a key difference is that software source code is formal since it is written in a programming language. The focus of this principle lies in the ability to share input and output with other software, thus incorporating the terms machine readability and data exchange. For I2, this principle is split into two sub-principles. The authors consider software and data it operates on as separate cases. \acrshort{fair} vocabulary should be used for both cases since \acrshort{fair} software should operate on \acrshort{fair} data. FAIRsharing.org provides a registry of vocabularies, data types, formats and schemas for software. For I3, the aim is the interconnection of data sets by semantically meaningful relationships, which is not directly applicable to research software. While software dependencies are somewhat similar, there is not much semantically meaningfulness required as the primary relationship is “dependsOn”.

Regarding \textbf{reusability}, the main goal is to have reproducible re-use of software within different scenarios \cite{benureau_re-run_2018}:
\begin{itemize}
    \item reproducing the same outputs as reported
    \item (re)using the software with different data
    \item (re)using the software for (unsupported) cases
    \item extending the software for additional functionality
\end{itemize}

Other relevant aspects are the previously explained software maintainability (documentation), the legality of reuse (licenses), and credit attribution to research software (citation). For R1.1, metadata should have separate licenses from software as they can be considered independent. License propagation is not needed for metadata. Software dependencies also make proper license management more complicated than for data since licenses of dependencies need to be considered. Machine readability becomes important to automatically determine incompatibilities of dependencies due to licensing. For R1.2, provenance refers to origin, source, and history of software and its associated metadata. Commonly included provenance data are contact information of the resource provider, publication date and location. Software versions are a minimum requirement to track the provenance of the software itself. Another aspect is the production of the software, for example, dependencies (see also I4S) and compilation of the software. Citation and contribution information is related to provenance.

Outside of the \acrshort{fair} principles, other considerations are community-specific metadata schemes for software that will play an important role, and the need for a governance model for the \acrshort{fair} principles to enable an open process for updating them. This should also be considered for the different domains within the scope of community discussions. The authors' work also provides the foundation for the development of metrics and maturity models to inform about software's FAIRness. 
The Netherlands eScience Center and DANS released five FAIR software recommendations \cite{noauthor_fair_nodate, martinez-ortiz_five_2020}. The Python package \textit{howfairis} can also check compliance with the five recommendations for GitHub repositories \cite{Spaaks_howfairis}. 
Both of these resources will be updated to be more in line with the \acrshort{fair4rs} principles \cite{martinez-ortiz_fair4rs_2022}. These \acrshort{fair} variables are also used in the \acrshort{swordsuu} framework and, therefore, in this study. However, the necessary level of FAIRness also depends on the type of software and should be discussed within the community. 


The \acrshort{fair4rs}-subgroup4 reviewed the position paper, including a survey regarding \acrshort{fair} principles for research software and suitability of the \acrshort{fair} principles proposed by Lamprecht et al. \cite{chue_hong_what_2021}. Having such a review allows further work to incorporate community feedback. One aspect that was often mentioned, especially in the question of what accessible and interoperable means in the context of research software, was sustainability. The principles had around 60 to 90 percent acceptance. Oftentimes, it was criticized what exactly is meant by the concrete principle or how it applies to research software, indicating that ambiguity needs to be minimized. There were further publications since the publication of the position paper that incorporated previous findings and community feedback to further develop \acrshort{fair} principles \cite{katz_fresh_2021, chue_hong_fair_2021, chue_hong_fair_2022}. 



The output of the previously mentioned \acrshort{fair4rs}-subgroups was discussed by Katz et al. \cite{katz_fair4rs_2021-1}, which included a community consultation. This resulted in five concerns regarding the scope and interpretation of terms that need to be addressed:
\begin{itemize}
    \item \textbf{General vs. specific principles}: There needs to be a balance between general and specific instructions. While actionable instructions are easier to act upon, they are also less long-lasting than general statements.
    \item \textbf{Long-term access to software}: Should this be relevant for FAIR Research Software Principles?
    \item \textbf{Defining research software}: There are two definitions that were considered, inclusive and exclusive. Inclusive refers to every software related to the research while exclusive refers to software only from reviewed publications.
    \item \textbf{Defining software}: Software can have many forms: scripts, executables, binaries, workflows and more. They suggest to define software as “A set of instructions that performs some action, either as source code (machine- and human-readable) or executable.”
    \item \textbf{FAIRness of related research objects}: How should FAIRness of related objects like software dependencies and documentation be handled? Is \acrshort{fair} recursively applicable?
\end{itemize}
Additionally, they looked at the different recommendations \cite{lamprecht_towards_2020, noauthor_fair_nodate, katz_fresh_2021} regarding their differences and agreement on the \acrshort{fair} principles. A summary of the findings can be seen in \autoref{tab:fairconsultation}.

\afterpage{
{\setlength\LTleft{-1.25cm}
\setlength\LTright{-1cm}
% \scriptsize
\footnotesize
\begin{longtable}{@{\extracolsep{\fill}} p{2cm}   p{7.5cm}  p{8.5cm}}
\textbf{Principle} & \textbf{Agreement} & \textbf{Discord} \\
\hline
\hline
Findable & All agree about the foundational principle and mostly agree about the guiding principles. & Minor. Some aspects like software granularity are only considered in some recommendations and levels of detail differ. \\ 
\hline
Accessible & All agree about the foundational principle and that metadata should always be accessible even after software becomes unavailable. & Differences in interpretation and mentions regarding (metadata) accessibility, software, retrievability, authentication and authorization. \\
\hline
Interoperable & Some agreement in partial aspects in the definition of interoperability, but strong discord. & The three angles described by Lamprecht et al. are not agreed upon by the different recommendations. There is also disagreement whether controlled vocabularies like the Citation File Format are relevant. \\
\hline
Reusable & All agree that a clear license is essential, detailed provenance is helpful, software should be described with accurate and relevant attributes, and that potential use and reuse should be maximized while following software engineering best practices. & Only some recommendations mention maintainability and dependability. Some interpret reuse as a much wider term. There is also disagreement about the relevance of executability of software, encapsulation and interpretation of community standards.


\caption{Agreement and discord of different recommendations \label{tab:fairconsultation}}
\end{longtable}}
}
Overall, interoperability and reusability are discussed and interpreted to varying levels of detail across the recommendations as captured in this community consultation. This largely stems from the different interpretations and meanings of these words that vary across research areas. There are differences even where the recommendations generally agree, like for findability. These findings indicate the need for researching characteristics for different domains to improve \acrshort{rse} practice. 

The \acrshort{fair4rs} Steering Committee drafted adoption guidelines \cite{chue_hong_fair_2021} that were reviewed by the community \cite{chue_hong_fair_2022}. For the development of the \acrshort{fair4rs} principles, the original \acrshort{fair} principles intention was taken as the starting point: “to maximize the added-value gained by contemporary, formal scholarly digital publishing” and “to ensure transparency, reproducibility, and reusability” \cite{wilkinson_fair_2016}. They also mentioned that the original \acrshort{fair} principles might need to be reinterpreted, that FAIRness is not binary, that software can appear in different forms like executables and binaries, and that the \acrshort{fair4rs} principles should be applied to all research software that is related to publications. Also, best practices in software engineering are relevant to the \acrshort{fair4rs} principles since some practices directly improve FAIRness. The application of \acrshort{fair4rs} principles is also the responsibility of software owners. 
The draft publication \cite{chue_hong_fair_2021} was reviewed by the community and has been superseded by an updated version that incorporates community feedback on the proposed \acrshort{fair4rs} principles for the release of version 1.0 \cite{chue_hong_fair_2022}. These can be seen in \autoref{AppendixA}. The authors provided an accompanying text for each foundational and guiding principle that explains how to interpret the meaning. In the context of software, interoperability and reusability, as defined in the original \acrshort{fair} principles, overlap. Therefore, interoperability focuses on data exchange between independent software. In contrast, reusability focuses on the ability of execution, inspection and understanding of humans and machines.
The draft publication includes two sections that talk about implementation challenges and what is necessary to adopt \acrshort{fair4rs} principles widely. The mentioned challenges hinder the adoption of the \acrshort{fair4rs} principles. They are about metadata and identifier authority, metadata vocabularies and properties, software identifiers, identification targets, software structure complexity, FAIRness of related research objects, definition of accessibility and reusability, and the relation of openness and \acrshort{fair}. 
%Relevant stakeholders for the adoption of \acrshort{fair4rs} principles include those that will endorse and promote adoption guidelines, as well as those that provide training and use them. 
They also provide a summary of what the European Commission deems necessary to support FAIR digital objects: metrics, incentives, skills and FAIR services \cite{directorate-general_for_research_and_innovation_european_commission_turning_2018}.


% ----------------------------------------------------------------- %
% ----------------------------------------------------------------- %
% ----------------------------------------------------------------- %
% ----------------------------------------------------------------- %
% \subsection{State of FAIRness and openness of research software}
% ----------------------------------------------------------------- %
% ----------------------------------------------------------------- %
% ----------------------------------------------------------------- %
% ----------------------------------------------------------------- %
Hasselbring et al. \cite{hasselbring_fair_2020} reviewed the current state of FAIRness and openness of research software. The authors consider the pragmatic and infrastructure views of the variations of open science. The pragmatic view has the central assumption that "Knowledge-creation could be more efficient if scientists worked together". In contrast, the infrastructure view assumes that “Efficient research depends on the available tools and applications“ \cite{fecher_open_2014}.
These views require research software engineering to be sustainable, which requires \acrshort{rse}s. This is a relatively new academic position. The \acrshort{rse} community displayed interest in promoting open science, which puts them in a perfect position to help researchers adopt \acrshort{fair} and open software practices.
A key obstacle in this endeavour is ensuring appropriate credit and recognition for the RSEs, which the authors recommend addressing via software citation and software observatories. Sustaining the communities, which include researchers, developers, maintainers, managers and active software users, is also essential.
Research software has not reached a similar level of FAIRness as research data. Software versions are often not archived, for example, via Zenodo, but only available on GitHub, which is not made for archiving. Journals often allow to add supplementary material to publications, but it is often not further reviewed or explained and therefore does not support reuse. However, software journals exist where research software might be published, for example, the Journal of Open Source Software or the Journal of Open Research Software \cite{noauthor_journal_nodate, noauthor_journal_nodate-1}. There is also the possibility of sharing research software services to allow for direct reuse of the software without the need for installation procedures \cite{noauthor_binderhub_nodate, noauthor_sobigdataeu_nodate}. They also state that research software has often been of subpar quality. This includes (no) testing, lacking documentation and coding standards which is a result of scientists not being judged based on the quality of the software that enables their publications. However, there have been initiatives including but not limited to several ACM conferences to start an artefact evaluation process, as well as the "Most Reproducible Paper Award" by ACM SIGMOD. The authors provide recommendations to address the issues of research software not being properly published for scientific reward and proper citation along the FAIR principles.
Challenges regarding \textbf{findability} are methods of software citation and software retrieval. A key challenge is appropriate software metadata. 
Some solutions regarding software citation and identification were previously mentioned \cite{noauthor_codemeta_2021, druskat_citation_2021}.
Software artefacts should be published with preservation in mind to increase \textbf{accessibility}. GitHub alone does not support preservation. A combination of GitHub for use, reuse and active involvement, and Zenodo for archival and reproducibility, achieves this \cite{noauthor_github_nodate}. Another example would be the Software Heritage archive \cite{di_cosmo_software_2017}.
Established software and data standards should be followed to increase \textbf{interoperability} and may be part of artefact evaluation processes. In addition, virtualization through Docker, for example, and online services improve interoperability and reusability across platforms.
Artefact evaluation processes should review replicability, reproducibility, and reusability of research software to increase \textbf{reusability}. Modular software allows for reuse of parts of the software, and good engineering practices ensure that other software can be built on top by others. These include, for example, proper documentation, providing testing frameworks, and test data for continuous integration.
Based on their experience with their analysis of the relationship between research publications and research software, the authors proposed the deployment of decentralized research software observatories, which would support research software retrieval and analysis. Such an observatory would provide support to open science research and encourage best practices among research communities. Additionally, it would allow exploring opportunities and challenges of cataloguing research software. Two critical components of such an observatory are support for metadata that allows classification of research software and citation to enable FAIRness. In that regard, they also mention the Research Software Directory \cite{spaaks_research_2020} as a related system. 
However, this system's focus is on repeatability and less on \acrshort{fair} and open principles. A first observatory for life sciences is already being developed \cite{martin_del_pico_automating_2020, pico_fairsoft_2022}.


Anzt et al. \cite{anzt_environment_2021} described the status and challenges of research software sustainability and suggested possible improvements with a focus on the German landscape. Common challenges for creating sustainable research software were described. 
They identified different stakeholders and where their interest stems from, ranging from the general public, geopolitical units, industry and independent developers to RSEs, domain researchers and research funding organizations. Since this study's goal is to improve an organization, it is of primary interest what motivates RSEs, research leaders, domain researchers, research funding organizations, and libraries which can be part of an organization. The common arguments for the motivation of sustainable research software for these people of interest are intrinsic interest in developing more sustainable and higher quality software, visibility and reputation, spending more time and funding on actual research and less on (re-)creating software, compliance with best practices, and reuse in other areas. The authors proposed a set of evaluation criteria for funding sustainable research software based on previously proposed evaluation schemes. The criteria are either hard criteria that everyone should fulfill or soft criteria that depend more on specific community standards. These are about usage and impact, software transparency and quality, and software maturity and are also part of a usual \acrfull{smp} \cite{noauthor_software_nodate}. It makes sense to focus on sustaining research software that is expected to have high usage and impact, good quality and high maturity to utilize available \acrshort{rse} resources best.


Gruenpeter et al. \cite{gruenpeter_morane_m215_2020} reviewed and analysed available \acrshort{fair} principles for research software and presented recommendations for constructing these principles. They stated that it needs to be taken into account that researchers already face significant challenges regarding research software development and maintenance. They also mentioned the need for training to produce \acrshort{fair} and citable software and that issues arise due to different interpretations for data and software, for example, regarding reuse and interoperability. 
The CURE-FAIR Working Group \cite{peer_challenges_2021} also reviewed computational reproducibility challenges and has determined sociocultural factors as one of the main challenges. This includes insufficient training and skills. Martinez-Ortiz et al. \cite{martinez-ortiz_fair4rs_2022} identified early adopters of the \acrshort{fair4rs} principles where common actions taken include training, metrics, guidelines and infrastructure \cite{martinez_survey_2022}. Especially the need for metrics to better quantify FAIRness has been emphasized, and a working group for metrics development was created \cite{barker_fair4rs_2022, noauthor_fair4rs_nodate, fair4rsmetricswg}. Pico et al. \cite{pico_fairsoft_2022} proposed first indicators for quantitative measurement of FAIRness.


During consultations and presentations in the context of the \acrshort{swordsuu} framework with different stakeholders, we noted that researchers indicated their approval of \acrshort{fair} principles for research software. However, their issue often was that they did not know how to make their research software more \acrshort{fair}. The main reason for this is that the principles are too abstract and not directly actionable, which further illustrates the need for appropriate training and resources. Researchers are typically not formally educated software engineers, and as mentioned, best practices often overlap with \acrshort{fair} principles. As an experiment, we contributed to research software repositories to improve their FAIRness, which was well received.


Since the first publications of \acrshort{fair} principles, various projects and initiatives have supported the facilitation of \acrshort{fair} research software \cite{van_lissa_worcs_2021, de_Bruin_Scan_and_revieW_2021, Spaaks_howfairis, ringersma_i-pass_2021, noauthor_codemeta_2021, martinez_survey_2022, ardcfairsoftware}.
What can be seen from the existing literature regarding \acrshort{fair} research software is that some things are generally agreed upon, which includes the provision of a license, rich metadata, the need for quantifiable metrics, possibility for citation, infrastructure, and training. On the other hand, there are differing opinions regarding interpretations of suggested principles, scope, and definitions. This indicates that there exists ambiguity that needs to be cleared or that terms are understood in different contexts depending on the domain.









\section{GitHub research software analyses regarding FAIRness and research domains}
\label{sec:lit_github}
There have been many analyses on GitHub data \cite{cosentino_systematic_2017}. However, none of them focused on developing recommendations of FAIRness for \acrshort{rse} practices on an organizational level. Their focus often lies on highly active projects, which is not our focus \cite{hu_influence_2016, borges_understanding_2016, blincoe_understanding_2016, sheoran_understanding_2014, ma_what_2016, kochhar_empirical_2013, ray_large_2014}. Research software might not have these characteristics. Since we are interested in the differences between research domains, we looked at GitHub analyses that concern themselves with domains. Additionally, analyses that suggested metrics regarding openness and sustainability were inspected, as these topics are related to FAIRness. 
Due to the topic of FAIR research software being relatively new, there are not many publications regarding FAIR research software analysis on GitHub compared to the amount of general GitHub analysis.

Gharehyazie et al. \cite{gharehyazie_here_2017} 
found that code reuse across projects in GitHub for \acrshort{foss} code is significant and more common within domain boundaries. These findings all point towards existing cross-domain differences and show that project characteristics differ between different areas. 

Russell et al. \cite{russell_large-scale_2018} mentioned that the background of researchers is very diverse, leading to a unique culture around programming within Bioinformatics, where repositories tend to be small with few contributors. They capture highly varying repositories for their analysis. Key findings include that around half of the repositories did not have a license and a lack of reuse.

Hasselbring et al. \cite{hasselbring_open_2020} conducted an analysis of research software in the research areas of computational science and computer science to provide recommendations to increase FAIRness and openness. They noted that the emphasis in computational science lies in reproducibility, while the emphasis in computer science lies in reuse. They follow that publication behaviour varies between research areas. The average life span of research software is defined as the time from the first to the last commit activity. This metric greatly differs between computer science and computational science with a 5-year median and 15-day median, respectively. 

Pico et al. \cite{pico_fairsoft_2022} proposed a software quality assessment framework for FAIRness using weighted indicators as a scoring tool that combined different (meta)data sources, including GitHub. This was tested on software within the life sciences, thus depicting an initial landscape of that domain and providing the first steps toward an observatory. As we have seen from Katz et al. \cite{katz_fair4rs_2021-1} in the previous section, there is unclarity regarding general and specific principles. This study is helpful as it provides specific and concrete things that researchers can scrutinize to improve FAIRness of their software. Their analysis found that research software in life sciences is highly findable, moderately accessible and reusable, and barely interoperable with their defined weights of the indicators. Findability being the highest makes sense, as otherwise, the research software would not be able to be part of the dataset. Interoperability also proved difficult to be translated into measurable indicators, raising the question of whether the measurability is the problem or if low interoperability is an actual finding. This is in line with what Lamprecht et al. \cite{lamprecht_towards_2020} mentioned, that interoperability is the most challenging high-level principle. Many indicators are related to specific resources within the life sciences, which illustrates that domain-specific indicators might be necessary. This becomes apparent if one would like to assess if accepted ontologies were used due to domain specificity or if the software is found in a domain-specific registry.

