\vspace{-1cm}
\chapter{Discussion}
\label{chap:discussion}
% show why the results "solve" the problem and to what extent,â€¯ describe what your research or design adds to the field's knowledge, what remains to be done and what other problems may be triggered; give hints for further study 
% \begin{itemize}
%     \item How can information about open source publications on GitHub be used to infer actionable recommendations for \acrshort{rse} practice to improve the research software landscape of an organization? 
% \end{itemize}

We argue how the research questions are answered based on the results in \autoref{chap:results}. Subquestion 1 is related to the literature review and will not be further discussed. The \acrshort{fair} variables that were retrieved from the literature review for subquestion 2 are validated in \autoref{sec:disc:sq2}. \autoref{sec:disc:sq3_sq4} answers subquestion 3 and 4 that were related to subpopulation characteristics and support for application of \acrshort{fair} variables. The last subquestion, which is about research software classification, is discussed in \autoref{sec:disc:sq5}. Lastly, we look at the limitations of our study in \autoref{sec:disc:limitations}.


% \vspace{-0.3cm}
\section{Validation of additional \acrshort{fair} variables}
\label{sec:disc:sq2}
%   \item How can FAIR principles be supplemented with additional variables?  
To validate \textbf{subquestion 2} (\textit{how can FAIR principles be supplemented with additional variables?}), we looked at the Jaccard similarity coefficient of the \acrshort{fair} variables in \autoref{fig:jaccard} and percentages for research and non-research software in \autoref{fig:heatmap_fair_booleans}. Since only a tiny percentage of repositories have information about registration, citation, and checklists, it is helpful to have additional measures of FAIRness. While all repositories should at least include license and citation information, it is not always sensible to register research software or scripts in a registry or to create a checklist. 
However, it can be argued that \acrshort{fair} variables should be applicable for all kinds of research software. Unlike the other proposed \acrshort{fair} variables, the commit-related variables \textit{correct vcs usage} and \textit{repository active} do not have a higher similarity with licensed repositories. This might indicate that these are not a good measure for FAIRness since licensing published software is a fundamental part of FAIRness. Additionally, while it might be a good practice in software development to frequently commit and correctly use version control, this aspect only relates marginally to \acrshort{fair} principles. Their primary focus lies on measuring openness and sustainability. However, \textit{correct vcs usage} might serve as a good measure to indicate which researchers might need support with the usage of software tools.

For the other \acrshort{fair} variables, they relate to a stronger extent to the \acrshort{fair} principles. Findability is improved through clear version identifiability, accessibility through contribution guidelines, reusability through install instructions, example usage, and tests. As such, we determine them as valuable additions as \acrshort{fair} variables. It does not seem to be useful to determine faculty-specific \acrshort{fair} variables. For example, even if Geosciences is significantly different to all other faculties regarding the number of contributors, it still makes sense to include contribution guidelines in case someone wants to contribute. Having such a guideline improves FAIRness, nonetheless.


%   \item How can the application of FAIR variables for research software be supported? 
%   \item Are there different characteristics for different subpopulations in the data?

% \vspace{-0.3cm}
\section{Subpopulations and supporting application of \acrshort{fair} variables}
\label{sec:disc:sq3_sq4}

To answer \textbf{subquestion 3} (\textit{are there different characteristics for different subpopulations in the data?}) and \textbf{subquestion 4} (\textit{how can the application of FAIR variables for research software be supported?}), we looked at descriptive statistics, licenses, languages, topics, and statistical test analysis. We first address \textbf{subquestion 3}. 

\autoref{fig:heatmap_numeric_variables} provided an overview of how available metrics differed between the faculties and repository types. Further faculty-specific differences could then be seen for both license and language usage. Social Sciences use R in more than 70\% of their repositories. In contrast, other faculties use mainly Python to a lesser degree and a mix of other languages. \autoref{fig:heatmap_fair_booleans} revealed more differences regarding \acrshort{fair} variables, with \autoref{fig:fair_score} revealing a potential order of measured FAIRness across the faculties. This also empowers the argument that the \acrshort{rse} community is in a perfect position to support the improvement of FAIRness \cite{hasselbring_fair_2020}. They are part of the \textit{Support department}, which scored highest on average. 
It should be noted that the \acrshort{fair} score from \autoref{fig:fair_score} is currently designed such that each \acrshort{fair} variable has an equal weight. We have already discussed the importance of having a license, which should therefore be weighted higher than having a checklist. Pico et al. \cite{pico_fairsoft_2022} implemented such a weighting for their analysis.
The statistical tests confirmed that all metrics except life span are significantly different. This is particularly interesting since Hasselbring et al. \cite{hasselbring_open_2020} found the life span median to be vastly different across the two subpopulations they compared. However, they only presented the median value without a statistical test for significance. 
The tests also showed that Geosciences are significantly different to all other faculties regarding contributors, which means that they usually have less expertise available, possibly leading to the lowest \acrshort{fair} score.
It might also be worthwhile to consider the further levels of subpopulations. We looked only at the level of faculties. While it is possible to go down further levels, it might also be of interest to group the employees via their job position, which is also available information. How does the research software published by a full professor differ from that of a research engineer or PhD student? Are the effects larger than across faculties? This allows us to understand whether position-based training or materials might be more appropriate than faculty-based training. 
Based on these findings, we conclude that there are different characteristics at the faculty level. 

This finding becomes relevant in answering \textbf{subquestion 4}. In order to support the application of FAIR variables, we need to consider the FAIRness maturity level of the researchers. As we have seen, this seems to vary across the faculties. We can specifically examine Geosciences in different regards. Looking at \textit{licenses} in \autoref{fig:heatmap_licenses}, we see that most repositories in this faculty had no license, while other faculties performed considerably better in that regard. \autoref{fig:heatmap_fair_booleans} also showed that Geosciences has a lower percentage of correct vcs usage. As we have previously discussed regarding subquestion 2, this variable relates to good practices in software engineering. As such, this might indicate that Geosciences require more fundamental training in this topic and hands-on usage of GitHub. Such training should still be open for anybody to participate in but could be more geared towards the academic needs of Geosciences. A document or video-based tutorial would possibly also be worthwhile to consider. However, while this may save resources, it also comes with drawbacks. A training for this purpose seems most suited as it allows inexperienced researchers to ask questions and receive appropriate feedback. Understanding the tools that are used during the process of creating research software is a prerequisite to creating \acrshort{fair} research software. 

Coming back to the licenses, there would ideally be no repository without a license since unlicensed software can not be legally reused for any purpose. Therefore, this topic should be made more aware across all faculties as it concerns them equally. However, it is also noteworthy that research-related repositories have more licenses on average compared to non-research-related repositories and previously conducted analyses such as Russell et al. \cite{russell_large-scale_2018}. 
While the significance of other FAIR-related aspects is less extreme, they follow a similar logic in being relevant for all faculties. For the \acrshort{fair} variables in this dataset, this concerns \textit{has citation, has install instructions, has example usage, has contribution guidelines, has tests, version identifiable}. We see varying percentages across the variables and faculties, ranging from 1\% to 66\%. As such, these are aspects that can be improved everywhere. This could be accomplished by faculty-agnostic training or a document regarding best \acrshort{fair} practices. A document has the advantage of being easily editable, such that new additions or changes in best \acrshort{fair} practices can be incorporated with little effort to avoid the provision of outdated information. 
In addition, a yearly report on FAIRness that analyzes changes per faculty, similar to this study, might be helpful in tracking the effect of the implemented measures. It might also incentivize researchers to improve FAIRness, as such a report should provide transparent criteria on how to achieve this. This has been a big hurdle in improving FAIRness for many researchers, as stated in our findings from \acrshort{swordsuu} consultations and presentations in \autoref{sec:lit_fairrs}. Such a yearly report also becomes feasible since this study provides a labelled dataset and code for the analysis. If the phases are repeated, only new repositories need to be labelled, which is a minor effort compared to having no previously labelled data. 

% Re: correlation metrics
% TODO: discussion --> using topics for findability to increase contributions might be helpful




%     \item How can information about open source publications on GitHub be used to infer actionable recommendations for \acrshort{rse} practice to improve the research software landscape of an organization? 

% \vspace{-0.3cm}
\section{Identifying research software}
\label{sec:disc:sq5}
%   \item How well can we identify research software with available data?
\autoref{sec:classification} answers subquestion 5 (\textit{how well can we identify research software with available data?}). We used logistic regression and random forest to predict the class of the repositories. Our results showed that both models outperform the majority class prediction. Since random forest performs better than logistic regression across all performance measures, we assume this model is better suited for our purposes, dataset size, and available variables. Using such a classification model can be useful for future tasks. Relating back to the yearly report, it would first be necessary to label newly collected repositories each time. Using the model for prediction as a first automated labelling step reduces manual labor and will help in making such a yearly report even more feasible. The manual labelling process is time-consuming and error-prone. It should therefore be supported by automated solutions as much as possible.

The used features for our models were all numeric or boolean variables. Further development on classifier models should consider including categorical variables, such as the given license or most used language. Also, it would be beneficial for future classification to develop more variables. One example would be to include the readme text as n-grams or other potential forms. The classification could additionally be done for each subpopulation. However, considering the amount of currently available repositories, we decided against this. This might be more suitable if future work can also incorporate previous university employees.

% \vspace{-0.3cm}
\section{Limitations of study}
\label{sec:disc:limitations}
This study only considers current researchers due to the API restrictions of the employee pages. If there were public access to previous employees, it would be possible to widen this study to include all previous employees. 
Another limitation is the sole focus on GitHub data. While it is the most popular platform, \acrshort{uu} also hosts its own GitLab server with active repositories. 
However, the highest number of stars for a repository is seven. This might indicate that most of the relevant research software is indeed found on other platforms.
Furthermore, the variable retrieval implementation can be improved to be more accurate. For example, the variable \textit{has citation} does not take into account if there is citation information in the readme, which is quite common. The variable \textit{has install instructions} only checks for mentions of the \textit{install} keyword in the readme, but could also take language-specific metadata into account. This includes \textit{setup.py, DESCRIPTION, pom.xml, and package.json} files for Python, R, Java, and JacaScript, respectively. Incorporating these files would also allow us to collect information on dependencies and versions, as Lamprecht et al. \cite{lamprecht_towards_2020} described.